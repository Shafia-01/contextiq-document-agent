# ContextIQ
## An Enterprise-Ready Document Q&A AI Agent

**ContextIQ** is an end‚Äëto‚Äëend Document Q&A agent that ingests multiple research PDFs, extracts structured content (text, tables, basic metadata), and answers grounded questions over those documents using LLM APIs (Gemini by default, with Groq as an optional fallback). It is built as a small but realistic production slice: FastAPI backend, Streamlit UI, local vector store, and optional ArXiv integration for paper discovery.

### 1. Problem Statement & Objective

- **Problem**: Enterprise teams sit on large collections of PDFs and research reports that are hard to search and reason over. Copy‚Äëpasting into a chat model leads to context loss, no source attribution, and high risk of hallucinations.
- **Objective**: Provide a **grounded, auditable Q&A agent** that:
  - Ingests multiple PDFs and related documents
  - Preserves **page‚Äëlevel context** and document metadata
  - Answers questions, summarizes content, and surfaces key metrics
  - Always shows **where** the answer came from (document + page)

### 2. Architecture Overview

- **Frontend (`frontend/app.py`)**: Streamlit chat UI for uploading documents, running ArXiv searches, and interacting with the Q&A agent.
- **Backend (`backend/`)**:
  - `main.py` ‚Äì FastAPI app exposing `/upload`, `/ask`, and `/arxiv_search`.
  - `ingest.py` ‚Äì PDF‚Äëfirst ingestion: text, page markers, images, tables, and chunking.
  - `qa.py` ‚Äì QA engine, LLM abstraction (Gemini/Groq), ArXiv utilities, and retrieval + answer generation.
  - `vectorstore.py` ‚Äì In‚Äëmemory cosine‚Äësimilarity vector store.
  - `llm_client.py` ‚Äì Single entrypoint for LLM clients; reads **API keys from environment variables** only and uses Gemini as the default provider.
- **Data (`data/`)**: Local storage for uploaded documents and extracted assets (tables/images).

End‚Äëto‚Äëend flow: **Upload ‚Üí Ingest & Chunk ‚Üí Embed ‚Üí Retrieve ‚Üí Generate Answer + Attribution ‚Üí Display in UI.**

### 3. Enterprise‚ÄëAligned Feature Map

- **Document ingestion**
  - PDF‚Äëoptimized pipeline using `fitz` and `pdfplumber`
  - Page‚Äëlevel markers (`[PAGE n]`) preserved to recover **page numbers per chunk**
  - Extraction of text, basic metadata, tables (CSV) and images (for audit/debug)
- **Retrieval & QA**
  - SentenceTransformers (`all-mpnet-base-v2`) for local embeddings (Groq path)
  - Configurable chunk size and overlap (defaults: **1500 chars / 200‚Äëchar overlap**) to balance context and recall
  - Supports **per‚Äëdocument** answers and **combined, cross‚Äëdocument** analysis
- **Source attribution & grounding**
  - For each answer, backend returns:
    - Document name/title
    - Approximate **page number** for supporting chunks
    - Retrieval scores and a simple **confidence label** (high/medium/low)
  - Streamlit UI surfaces this alongside the natural‚Äëlanguage answer to reduce hallucination risk.
- **Security & operations**
  - API keys are **never hard‚Äëcoded**; they are read from env vars (`GEMINI_API_KEY`, `GROQ_API_KEY`, optional `LLM_PROVIDER`).
  - **Default provider**: Gemini. You can override with `LLM_PROVIDER=groq` if needed.
  - In‚Äëmemory vector store kept deliberately simple for review; can be swapped for FAISS/Pinecone/Chroma.

### 4. Quick Start

- **Prerequisites**
  - Python 3.8+
  - At least one LLM provider key: **Gemini** (default) and optionally **Groq**

- **Setup**

```bash
git clone https://github.com/Shafia-01/contextiq-document-agent
cd ContextIQ

pip install -r requirements.txt

# .env (not committed) ‚Äì example
export GEMINI_API_KEY=your_gemini_api_key_here   # default provider
export GROQ_API_KEY=your_groq_api_key_here       # optional fallback
export LLM_PROVIDER=gemini                       # or "groq"
```

- **Run backend**

```bash
cd backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

- **Run frontend**

```bash
cd ../frontend
streamlit run app.py --server.port 8501
```

- **Access**
  - UI: `http://localhost:8501`
  - API docs: `http://localhost:8000/docs`

### 5. Example Run (Happy Path)

1. **Upload documents**
   - In the sidebar, upload several research PDFs (e.g. transformer papers).
   - Backend extracts text + tables, chunks into overlapping spans, and populates the vector store.
2. **Ask questions**
   - Example queries:
     - *‚ÄúWhat is the main research question of each paper?‚Äù*
     - *‚ÄúAcross all papers, summarize how evaluation metrics are defined.‚Äù*
     - *‚ÄúOn which page do they report BLEU and ROUGE scores?‚Äù*
3. **Expected output (simplified)**
   - Natural‚Äëlanguage answer, e.g.:
     - *‚ÄúPaper A studies vocabulary growth in large LMs; Paper B proposes a new metric for ‚Ä¶‚Äù*
   - Plus **attribution section**, e.g.:
     - `Paper A (2509.00516v1.pdf) ‚Äì pages 3‚Äì4 (high similarity)`
     - `Paper B (571e56b308aeaced7889e0bd.pdf) ‚Äì page 7 (medium similarity)`
   - Confidence label: `confidence: medium (max similarity‚âà0.83, avg‚âà0.55)`.

You can also hit the API directly via `/upload`, `/ask`, and `/arxiv_search` using `curl` or Postman.

### 6. PDF Ingestion & Edge Cases

- **Text extraction**
  - Uses `fitz` to iterate pages; each page is logged and wrapped with a `[PAGE n]` marker so later chunks can be mapped back to pages.
  - `pdfplumber` is used opportunistically to extract tables per page to CSV.
- **Chunking strategy**
  - Greedy character‚Äëbased chunks of ~1500 chars with 200‚Äëchar overlap.
  - This size is chosen to:
    - Fit comfortably within LLM context windows
    - Preserve enough local context for section‚Äëlevel questions
    - Avoid exploding the number of embeddings for large documents
- **Edge‚Äëcase handling (by design)**
  - **Very large PDFs**: chunking is capped via `max_chunks` (default 500) to prevent unbounded memory growth.
  - **Scanned/broken PDFs**: pages with very low text content are logged; behavior is to ingest what‚Äôs available and surface the limitation in logs (a realistic place to plug in OCR later).
  - **Empty pages**: explicitly detected and skipped in effective context while keeping page numbering consistent.

### 7. Evaluation & Sanity Checks

This project includes a **lightweight, explainable evaluation heuristic** rather than a heavy benchmark:

- During retrieval, each chunk receives a cosine similarity score to the query.
- The QA engine computes:
  - `max_score` and `avg_score` across retrieved chunks
  - A human‚Äëfriendly **confidence label**:
    - High: strong, consistent similarity
    - Medium: mixed evidence
    - Low: weak grounding ‚Äì likely to contain hallucinations or off‚Äëtopic content
- Both the UI and API surface this label and the supporting documents/pages used.

This mirrors how an enterprise system might surface ‚Äúanswer quality‚Äù without hiding the underlying retrieval evidence.

## üîó [Live Demo](https://example.com)
